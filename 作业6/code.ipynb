{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from collections import Counter\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Desktop\\recent\\研一上\\学习\\python\\闫嘉依 作业6\n",
      "yob1880.txt\n",
      "yob1881.txt\n",
      "yob1882.txt\n",
      "yob1883.txt\n",
      "yob1884.txt\n",
      "yob1885.txt\n",
      "yob1886.txt\n",
      "yob1887.txt\n",
      "yob1888.txt\n",
      "yob1889.txt\n",
      "yob1890.txt\n",
      "yob1891.txt\n",
      "yob1892.txt\n",
      "yob1893.txt\n",
      "yob1894.txt\n",
      "yob1895.txt\n",
      "yob1896.txt\n",
      "yob1897.txt\n",
      "yob1898.txt\n",
      "yob1899.txt\n",
      "yob1900.txt\n",
      "yob1901.txt\n",
      "yob1902.txt\n",
      "yob1903.txt\n",
      "yob1904.txt\n",
      "yob1905.txt\n",
      "yob1906.txt\n",
      "yob1907.txt\n",
      "yob1908.txt\n",
      "yob1909.txt\n",
      "yob1910.txt\n",
      "yob1911.txt\n",
      "yob1912.txt\n",
      "yob1913.txt\n",
      "yob1914.txt\n",
      "yob1915.txt\n",
      "yob1916.txt\n",
      "yob1917.txt\n",
      "yob1918.txt\n",
      "yob1919.txt\n",
      "yob1920.txt\n",
      "yob1921.txt\n",
      "yob1922.txt\n",
      "yob1923.txt\n",
      "yob1924.txt\n",
      "yob1925.txt\n",
      "yob1926.txt\n",
      "yob1927.txt\n",
      "yob1928.txt\n",
      "yob1929.txt\n",
      "yob1930.txt\n",
      "yob1931.txt\n",
      "yob1932.txt\n",
      "yob1933.txt\n",
      "yob1934.txt\n",
      "yob1935.txt\n",
      "yob1936.txt\n",
      "yob1937.txt\n",
      "yob1938.txt\n",
      "yob1939.txt\n",
      "yob1940.txt\n",
      "yob1941.txt\n",
      "yob1942.txt\n",
      "yob1943.txt\n",
      "yob1944.txt\n",
      "yob1945.txt\n",
      "yob1946.txt\n",
      "yob1947.txt\n",
      "yob1948.txt\n",
      "yob1949.txt\n",
      "yob1950.txt\n",
      "yob1951.txt\n",
      "yob1952.txt\n",
      "yob1953.txt\n",
      "yob1954.txt\n",
      "yob1955.txt\n",
      "yob1956.txt\n",
      "yob1957.txt\n",
      "yob1958.txt\n",
      "yob1959.txt\n",
      "yob1960.txt\n",
      "yob1961.txt\n",
      "yob1962.txt\n",
      "yob1963.txt\n",
      "yob1964.txt\n",
      "yob1965.txt\n",
      "yob1966.txt\n",
      "yob1967.txt\n",
      "yob1968.txt\n",
      "yob1969.txt\n",
      "yob1970.txt\n",
      "yob1971.txt\n",
      "yob1972.txt\n",
      "yob1973.txt\n",
      "yob1974.txt\n",
      "yob1975.txt\n",
      "yob1976.txt\n",
      "yob1977.txt\n",
      "yob1978.txt\n",
      "yob1979.txt\n",
      "yob1980.txt\n",
      "yob1981.txt\n",
      "yob1982.txt\n",
      "yob1983.txt\n",
      "yob1984.txt\n",
      "yob1985.txt\n",
      "yob1986.txt\n",
      "yob1987.txt\n",
      "yob1988.txt\n",
      "yob1989.txt\n",
      "yob1990.txt\n",
      "yob1991.txt\n",
      "yob1992.txt\n",
      "yob1993.txt\n",
      "yob1994.txt\n",
      "yob1995.txt\n",
      "yob1996.txt\n",
      "yob1997.txt\n",
      "yob1998.txt\n",
      "yob1999.txt\n",
      "yob2000.txt\n",
      "yob2001.txt\n",
      "yob2002.txt\n",
      "yob2003.txt\n",
      "yob2004.txt\n",
      "yob2005.txt\n",
      "yob2006.txt\n",
      "yob2007.txt\n",
      "yob2008.txt\n",
      "yob2009.txt\n",
      "yob2010.txt\n",
      "yob2011.txt\n",
      "yob2012.txt\n",
      "yob2013.txt\n",
      "yob2014.txt\n",
      "yob2015.txt\n",
      "yob2016.txt\n",
      "yob2017.txt\n",
      "yob2018.txt\n",
      "yob2019.txt\n",
      "yob2020.txt\n",
      "finish\n",
      "C:\\Users\\DELL\\Desktop\\recent\\研一上\\学习\\python\\闫嘉依 作业6\n"
     ]
    }
   ],
   "source": [
    "#读取数据\n",
    "#到dataset文件夹下，创建all.txt\n",
    "print(os.path.abspath(os.path.join(os.getcwd())))\n",
    "all_file = open('./dataset/all.txt','a')\n",
    "\n",
    "#到names文件夹下，获取所有txt的文件名\n",
    "os.chdir('./dataset/names')\n",
    "files=glob.glob('*.txt')\n",
    "\n",
    "#将每个txt写入all.txt中\n",
    "for filename in files:  \n",
    "    print(filename)  \n",
    "    fopen=open(filename,'r',encoding='utf-8')  \n",
    "    lines=[]  \n",
    "    lines=fopen.readlines()  \n",
    "    fopen.close()  \n",
    "    i=0  \n",
    "    for line in lines:   \n",
    "        all_file.write(filename[3:-4]+',')\n",
    "        for x in line:  \n",
    "            all_file.write(x)\n",
    "        \n",
    "all_file.close()\n",
    "print('finish')\n",
    "\n",
    "#返回代码所在文件夹（上两级）\n",
    "os.chdir(os.path.join(os.getcwd(), \"../..\"))\n",
    "print(os.path.abspath(os.path.join(os.getcwd())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   birth       name sex  freq\n",
      "0   1880       Mary   F  7065\n",
      "1   1880       Anna   F  2604\n",
      "2   1880       Emma   F  2003\n",
      "3   1880  Elizabeth   F  1939\n",
      "4   1880     Minnie   F  1746\n",
      "(2020863, 4)\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv('./dataset/all.txt',sep=',',names=['birth','name','sex','freq'])\n",
    "print(data.head(5))\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'F': 1192115, 'M': 828748})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(data['sex']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name\n",
      "Aaban      120\n",
      "Aabha       46\n",
      "Aabid       16\n",
      "Aabidah      5\n",
      "Aabir       10\n",
      "Name: freq, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "names=data.groupby(by=['name'])['freq'].sum()\n",
    "print(names.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name\n",
       "James      5213689\n",
       "John       5163958\n",
       "Robert     4849738\n",
       "Michael    4405274\n",
       "William    4159868\n",
       "Name: freq, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names.sort_values(ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100364\n"
     ]
    }
   ],
   "source": [
    "print(data['name'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feature1', 'feature2', 'feature3', 'feature4', 'feature5', 'feature6', 'feature7', 'feature8', 'feature9']\n",
      "['id', 'feature1', 'feature2', 'feature3', 'feature4', 'feature5', 'feature6', 'feature7', 'feature8', 'feature9', 'y']\n"
     ]
    }
   ],
   "source": [
    "'''生成列名'''\n",
    "#生成特征列名\n",
    "feature_names=['feature'+(str(i+1)) for i in range(9)]\n",
    "print(feature_names)\n",
    "#添加id列和标签列y\n",
    "col_names=[['id'],feature_names,['y']]\n",
    "#列表展开\n",
    "col_names=[i for k in col_names for i in k]\n",
    "print(col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(699, 11)\n",
      "        id  feature1  feature2  feature3  feature4  feature5 feature6  \\\n",
      "0  1000025         5         1         1         1         2        1   \n",
      "1  1002945         5         4         4         5         7       10   \n",
      "2  1015425         3         1         1         1         2        2   \n",
      "3  1016277         6         8         8         1         3        4   \n",
      "4  1017023         4         1         1         3         2        1   \n",
      "\n",
      "   feature7  feature8  feature9  y  \n",
      "0         3         1         1  2  \n",
      "1         3         2         1  2  \n",
      "2         3         1         1  2  \n",
      "3         3         7         1  2  \n",
      "4         3         1         1  2  \n"
     ]
    }
   ],
   "source": [
    "'''读取数据'''\n",
    "data = pd.read_csv('./dataset/breast-cancer-wisconsin.data', sep=',',names=col_names)\n",
    "print(data.shape)\n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x dim: (699, 8) y dim: (699,)\n",
      "Counter({2: 458, 4: 241})\n",
      "x train dim: (559, 8) x test dim: (140, 8) y train dim: (559,) y test dim: (140,)\n"
     ]
    }
   ],
   "source": [
    "'''数据预处理'''\n",
    "#缺失值替换为0\n",
    "data=data.replace('?',0)\n",
    "\n",
    "#拆分x,y\n",
    "x = data.iloc[:,1:-2]\n",
    "y=data.iloc[:,-1]\n",
    "print('x dim:',x.shape,'y dim:',y.shape)\n",
    "print(Counter(y))\n",
    "\n",
    "#分测试集训练集\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, shuffle=True, random_state=0, test_size=0.2)\n",
    "print('x train dim:',x_train.shape,'x test dim:',x_test.shape,'y train dim:',y_train.shape,'y test dim:',y_test.shape)\n",
    "\n",
    "#标准化处理\n",
    "transfer = StandardScaler()\n",
    "x_train = transfer.fit_transform(x_train)\n",
    "x_test = transfer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计时器\n",
    "def func(f):\n",
    "    def inner(*args,**kwargs):\n",
    "        start_time = time.time()\n",
    "        f(*args,**kwargs)\n",
    "        end_time=time.time()\n",
    "        print('耗时%s秒'%(end_time-start_time))\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#逻辑回归（手动实现）\n",
    "class MyLogisticRegression:\n",
    "    def __init__(self,learning_rate=0.001,max_iter=5000):\n",
    "        self._theta=None\n",
    "        self.intercept_=None\n",
    "        self.coef_=None\n",
    "        self.learning_rate=learning_rate\n",
    "        self.max_iter=max_iter\n",
    "    \n",
    "    def _sigmoid(self,z):\n",
    "        return 1./(1.+np.exp(-z))\n",
    "    \n",
    "    def fit(self,x_train,y_train):\n",
    "        def J(theta, X_b, y_train):\n",
    "            y_hat=self._sigmoid(X_b.dot(theta))\n",
    "            return -np.sum(y_train*np.log(y_hat)+(1-y_train)*np.log(1-y_hat))/len(y_train)\n",
    "        \n",
    "        def dJ(theta, X_b, y_train):\n",
    "            y_hat=self._sigmoid(X_b.dot(theta))\n",
    "            return X_b.T.dot(y_hat-y_train)/len(y_train)\n",
    "                \n",
    "        X_b=np.hstack([np.ones((len(x_train),1)), x_train])#生成偏置b\n",
    "        self._theta=np.random.randn(X_b.shape[1]) #生成权重theta，正态分布随机初始化\n",
    "        #梯度下降法优化\n",
    "        iter_num = 0\n",
    "        while iter_num<self.max_iter:\n",
    "            iter_num+=1\n",
    "            last_theta=self._theta\n",
    "            self._theta=self._theta-self.learning_rate*dJ(self._theta,X_b,y_train)\n",
    "            if (abs(J(self._theta,X_b,y_train)-J(last_theta,X_b,y_train))<1e-7):\n",
    "                break\n",
    "        \n",
    "        self.intercept_=self._theta[0]\n",
    "        self.coef_=self._theta[1:]\n",
    "        return self\n",
    "    \n",
    "    def predict(self,x_predict):\n",
    "        X_b=np.hstack([np.ones((len(x_predict), 1)), x_predict])\n",
    "        y_predict=self._sigmoid(X_b.dot(self._theta))#得到预测结果\n",
    "        y_predict=np.array(y_predict>=0.5 , dtype = 'int')#y_predict为0/1，但标签为2/4，因此需要做y=2*y+2的变换\n",
    "        return y_predict\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"LogisticRegression()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "耗时0.006980180740356445秒\n",
      "classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          良性       0.99      0.96      0.98        85\n",
      "          恶性       0.95      0.98      0.96        55\n",
      "\n",
      "    accuracy                           0.97       140\n",
      "   macro avg       0.97      0.97      0.97       140\n",
      "weighted avg       0.97      0.97      0.97       140\n",
      "\n",
      "auc: 0.9732620320855616\n"
     ]
    }
   ],
   "source": [
    "'''逻辑回归（调库）'''\n",
    "#拟合\n",
    "estimator=LogisticRegression()\n",
    "@func\n",
    "def numpy_LR():\n",
    "    estimator.fit(x_train, y_train)\n",
    "numpy_LR()\n",
    "\n",
    "#预测\n",
    "y_pre=estimator.predict(x_test)\n",
    "\n",
    "#评估\n",
    "report1=classification_report(y_test, y_pre,labels=(2, 4), target_names=(\"良性\", \"恶性\"))\n",
    "\n",
    "auc=roc_auc_score(y_test, y_pre)\n",
    "print('classification report:\\n',report1)\n",
    "print('auc:',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\.conda\\envs\\pytorch2\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: divide by zero encountered in log\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\DELL\\.conda\\envs\\pytorch2\\lib\\site-packages\\ipykernel_launcher.py:30: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "耗时11.339664697647095秒\n",
      "classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          良性       1.00      0.64      0.78        85\n",
      "          恶性       0.64      1.00      0.78        55\n",
      "\n",
      "    accuracy                           0.78       140\n",
      "   macro avg       0.82      0.82      0.78       140\n",
      "weighted avg       0.86      0.78      0.78       140\n",
      "\n",
      "auc: 0.8176470588235294\n"
     ]
    }
   ],
   "source": [
    "'''逻辑回归（手动实现）'''\n",
    "#拟合\n",
    "estimator=MyLogisticRegression()\n",
    "@func\n",
    "def my_LR():\n",
    "    estimator.fit(x_train, y_train)\n",
    "my_LR()\n",
    "\n",
    "#预测\n",
    "y_pre=estimator.predict(x_test)\n",
    "\n",
    "#评估\n",
    "report1=classification_report(y_test, y_pre,labels=(2, 4), target_names=(\"良性\", \"恶性\"))\n",
    "auc=roc_auc_score(y_test, y_pre)\n",
    "\n",
    "print('classification report:\\n',report1)\n",
    "print('auc:',auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
